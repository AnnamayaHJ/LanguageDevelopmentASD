---
title: "Assignment 1 - Language Development in ASD - part 3"
author: "Riccardo Fusaroli"
date: "August 10, 2018"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
getwd()
library(pacman)
p_load(lmerTest, pastecs, ggplot2, tidyverse, gdata, MuMIn, effects, stringr, plyr, vtreat, cvTools, caret)
```

```{r, include=FALSE}
#Loading the new data 
demo_test <- read.csv("demo_test.csv")
token_test <- read.csv("token_test.csv")
lu_test <- read.csv("lu_test.csv")
```

```{r, include=FALSE}
#Renaming the data sets to have the same headline for the subject = ID
demo_test <- rename.vars(demo_test, "Child.ID", "ID")
lu_test <- rename.vars(lu_test, "SUBJ", "ID")
token_test <- rename.vars(token_test, "SUBJ", "ID")

#Renaming the visit to have the same 
lu_test <- rename.vars(lu_test, "VISIT", "visit")
token_test <- rename.vars(token_test, "VISIT", "visit")
demo_test <- rename.vars(demo_test, "Visit", "visit")

#Homogeneize the way visit is reported, use of stringr to extract only numbers
lu_test$visit <- str_extract(lu_test$visit, "[1-6]")
token_test$visit <- str_extract(lu_test$visit, "[1-6]")

#Removing all punctuations in the ID column 
demo_test$ID <- str_replace_all(demo_test$ID, "[:punct:]", "")
lu_test$ID <- str_replace_all(lu_test$ID, "[:punct:]", "")
token_test$ID <- str_replace_all(token_test$ID, "[:punct:]", "")

#Selecting the variables needed
demo_test_sub <- select(demo_test, c(ID, visit, Ethnicity, Diagnosis, Gender, Age, ADOS,  MullenRaw, ExpressiveLangRaw))
lu_test_sub <- select(lu_test, c(ID, visit, MOT_MLU, MOT_LUstd, CHI_MLU, CHI_LUstd))
token_test_sub <- select(token_test, c(ID, visit, types_MOT, types_CHI, tokens_MOT, tokens_CHI))

library(plyr)
#Using join to merge to three datasets by ID and Visit
test <- join(demo_test_sub, lu_test_sub)
test <- join(test, token_test_sub)

#Ados for only visit 1
#Using subset to make a dataset only consisting of the data from visit 1
subset_test <- subset(test, visit == 1, select = c(ID, ADOS, MullenRaw, ExpressiveLangRaw))

#Changing the names in the new data set 
subset_test <- rename.vars(subset_test, c("ADOS", "MullenRaw", "ExpressiveLangRaw"), c("ADOS1", "NonverbalIQ", "VerbalIQ"))

#Merging the two datasets using join 
test <- join(test, subset_test)

#Using select to make a clean dataset with variables in the right order
clean_test <- select(test, c(ID, Diagnosis, visit, Gender, Age, Ethnicity,  ADOS1, NonverbalIQ, VerbalIQ, CHI_MLU, MOT_MLU, CHI_LUstd, MOT_LUstd, types_CHI, types_MOT,  tokens_CHI, tokens_MOT))

#Anonymous kids
clean_test$ID <- as.factor(clean_test$ID)
clean_test$ID <- as.numeric(clean_test$ID)

#Turning the 1s and 2s into M and F in the gender variable
clean_test$Gender <- ifelse(clean_test$Gender == "1", "M", "F")

#Turning the As and Bs into ADS and TD in the diagnosis variable using ifelse (short way)
clean_test$Diagnosis <- ifelse(clean_test$Diagnosis == "A", "ASD", "TD")

#Making a csv. file named LanguageASD.csv to WD
write.csv(clean_test, file = "test_data_clean_v1.csv")

```

```{r, include=FALSE}
#Loading data (if cleared workspace)
data_test <- read.csv("test_data_clean_v1.csv")

#Loading old data
data = read.csv("LanguageASD_final.csv")
```

```{r,include=FALSE}
data <- rename.vars(data, "Diagnosis", "diagnosis")
data <- rename.vars(data, "Gender", "gender")
data <- rename.vars(data, "Age", "age")
data <- rename.vars(data, "Ethnicity", "ethnicity")
data <- rename.vars(data, "Visit", "visit")
data <- rename.vars(data, "ADOS1", "ados1")
data <- rename.vars(data, "MullenRaw1", "nonverbalIQ")
data <- rename.vars(data, "ExpressiveLangRaw1", "verbalIQ")

data <- data[-c (1, 68, 69, 130, 131, 132, 276, 277, 284, 285), ]
```

```{r, include=FALSE}
data_test <- rename.vars(data_test, "Diagnosis", "diagnosis")
data_test <- rename.vars(data_test, "Gender", "gender")
data_test <- rename.vars(data_test, "Age", "age")
data_test <- rename.vars(data_test, "Ethnicity", "ethnicity")
data_test <- rename.vars(data_test, "ADOS1", "ados1")
data_test <- rename.vars(data_test, "NonverbalIQ", "nonverbalIQ")
data_test <- rename.vars(data_test, "VerbalIQ", "verbalIQ")
```

### Exercise 1) Testing model performance
```{r, include=FALSE}
#data$CHI_MLU <- data$CHI_MLU[!is.na(data$CHI_MLU)]
p_load(Metrics)

#Removing NAs in data sets
data <- na.omit(data, cols = "CHI_MLU")
data_test <- na.omit(data_test, cols = "CHI_MLU")

#Using our best model from the last assignment
model_train <- lmer(CHI_MLU ~ visit + I(visit^2) + ados1 + MOT_MLU + verbalIQ + (1+visit + I(visit^2) | ID), data = data, REML=F)

#Root mean square error on how the model perform on the training data
rmse(data$CHI_MLU, predict(model_train))
##[1] 0.2775267

#Very bad prediction on the test set 
rmse(data_test$CHI_MLU, predict(model_train, data_test))
##[1] 0.7683667
```

#### ANSWER: 
Our selected model from last assignment (pseudocode: mean length of utterance ~ visit + visit^2 + ados + mother_mlu + (visit + visit^2 | ID)) produces a root mean square error of 0.278. The root mean square error increases quite a lot when applying the model on the test set, from 0.278 to 0.768. We thus observe that the accuracy of the predictions decreases a lot when the model is applied to the new (test) data, indicating that the model's predictive power is quite low. As such, this model seem to contain a fair amount explanatory power, but the predictive power is low.

### Exercise 2) Model Selection via Cross-validation (N.B: ChildMLU!)
```{r, include=FALSE}
#Creating the basic model
hercules <- lmer(CHI_MLU ~ visit + diagnosis + (1 + visit | ID), data = data, REML = F)

#Creating our model from last week
zeus <- lmer(CHI_MLU ~ visit + I(visit^2) + ados1 + MOT_MLU + verbalIQ + (1+visit + I(visit^2)| ID), data = data, REML=F)
```

```{r, include=FALSE}
#Cross-validation of the basic model
set.seed(242)
folds <- createFolds(unique(data$ID),5)

n=1
rmsetrain = NULL
rmsetest = NULL

for (f in folds){
  train_data <- subset(data, !(ID %in% f))
  test_data <- subset(data, ID %in% f)
  model <- lmer(CHI_MLU ~ visit + diagnosis + (1 + visit| ID), data = train_data, REML = FALSE)
  rmsetrain[n] = rmse(train_data$CHI_MLU, predict(model))
  rmsetest[n] = rmse(test_data$CHI_MLU, predict(model, test_data, allow.new.levels = TRUE))
  n = n +1
}

# RMSE of model prediction when fitted to the full dataset
rmse(data$CHI_MLU, predict(hercules))
#[1] 0.3380725

# RMSE of model prediction using cross-validation
rmse_basic <- data.frame(rmsetest, rmsetrain)
mean(rmse_basic$rmsetest)
#[1] 0.8116268

#Cross-validation of our model
n=1
rmsetrain = NULL
rmsetest = NULL

for (f in folds){
  train_data <- subset(data, !(ID %in% f))
  test_data <- subset(data, ID %in% f)
  model <- lmer(CHI_MLU ~ visit + I(visit^2) + ados1 + MOT_MLU + verbalIQ + (1 + visit + I(visit^2) | ID), data = train_data, REML=FALSE)
  rmsetrain[n] = rmse(train_data$CHI_MLU, predict(model))
  rmsetest[n] = rmse(test_data$CHI_MLU, predict(model, test_data, allow.new.levels = TRUE))
  n = n +1
}

# RMSE of model prediction when fitted to the full dataset
rmse(data$CHI_MLU, predict(zeus))
#[1] 0.2775267

# RMSE of model prediction using cross-validation
rmse_fav <- data.frame(rmsetest, rmsetrain)
mean(rmse_fav$rmsetest)
#[1] 0.6341372
```

```{r, include=FALSE}
# Model predictions when using the separate test-data
rmse(predict(hercules, data_test), data_test$CHI_MLU)
#[1] 1.068573

rmse(predict(zeus, data_test), data_test$CHI_MLU)
#[1] 0.7683667
```

#### ANSWER: 
The basic linear model (pseudocode: mean length of utterance ~ visit + diagnosis + (visit | ID)) produces a root mean squared error of only 0.34 when fitted to the full dataset (test data excluded). However, when the model is trained and tested using 5-fold cross validation, the root mean squared error increases to 0.79.

As previously mentioned, our selected model (pseudocode: mean length of utterance ~ visit + visit^2 + verbalIQ + ados + mother's MLU (visit + visit^2 | ID)) produces a root mean squared error of only 0.28 when fitted to the full dataset (test data excluded). As with the linear model, the error increases when the model is trained and tested using 5-fold cross validation, in this case to roughly 0.62.

First of all, this means that when cross-validation our selected model remains superior in making predictions compared to the linear model.

When the two model predictions are applied to the actual test data they both turn out to be quite far off. The linear model produces a root mean squared error of 1.07 while the quadratic model produces a RMSE of 0.77. Thus, the quadratic model is still performing better on the test data than the linear, however somewhat worse than predicted with the training data (both with and without cross validation of the model, but especially the former).

```{r, include=FALSE}
#Testing Nanna's random model
n=1
rmsetrain = NULL
rmsetest = NULL

for (f in folds){
  train_data <- subset(data, !(ID %in% f))
  test_data <- subset(data, ID %in% f)
  model_curls <- lmer(CHI_MLU ~ visit + I(visit^2) + ados1 + verbalIQ + MOT_MLU + types_CHI + (1+visit + I(visit^2)| ID), data = train_data, REML = FALSE)
  rmsetrain[n] = rmse(train_data$CHI_MLU, predict(model_curls))
  rmsetest[n] = rmse(test_data$CHI_MLU, predict(model_curls, test_data, allow.new.levels = TRUE))
  n = n +1
}

rmse_curls <- data.frame(rmsetest, rmsetrain)
mean(rmse_curls$rmsetest)
#[1] 0.4777694

#Quadratic interaction with gender
n=1
rmsetrain = NULL
rmsetest = NULL

for (f in folds){
  train_data <- subset(data, !(ID %in% f))
  test_data <- subset(data, ID %in% f)
  model_g <- lmer(CHI_MLU ~ visit * diagnosis + I(visit^2) + verbalIQ + (1+visit + I(visit^2) | gender), data = train_data,  REML=FALSE)
  rmsetrain[n] = rmse(train_data$CHI_MLU, predict(model_g))
  rmsetest[n] = rmse(test_data$CHI_MLU, predict(model_g, test_data, allow.new.levels = TRUE))
  n = n +1
}

rmse_g <- data.frame(rmsetest, rmsetrain)
mean(rmse_g$rmsetest)
#[1] 0.5822574

#P-hacked model
n=1
rmsetrain = NULL
rmsetest = NULL

for (f in folds){
  train_data <- subset(data, !(ID %in% f))
  test_data <- subset(data, ID %in% f)
  model_p <- lmer(CHI_MLU ~ visit + I(visit^2) + nonverbalIQ + verbalIQ + types_CHI + MOT_MLU + (1+visit + I(visit^2) | ID), data = train_data, REML=FALSE)
  rmsetrain[n] = rmse(train_data$CHI_MLU, predict(model_p))
  rmsetest[n] = rmse(test_data$CHI_MLU, predict(model_p, test_data, allow.new.levels = TRUE))
  n = n +1
}

rmse_p <- data.frame(rmsetest, rmsetrain)
mean(rmse_p$rmsetest)
#[1] 0.460752
```

#### ANSWER:
Using Cross Validation and root mean squared error when applying the actual test data, the best predictive model attempted was our (somewhat p-hacked) model from the previous assignment (pseudocode: Child MLU ~ visit + I(visit^2) + nonverbalIQ + verbalIQ + types_CHI + MOT_MLU + (1+visit + I(visit^2) | ID), producing a RMSE of 0.46. Interestingly, this model does not include ados or diagnosis. It also completely ignores the previously posed hypotheses. Should one prefer to stay true to the hypotheses, another model (pseudocode: CHI_MLU ~ visit + I(visit^2) + ados1 + verbalIQ + MOT_MLU + types_CHI + (1+visit + I(visit^2) | ID) performs nearly as well with a RMSE of 0.48. 

### Exercise 3) Assessing the single child
```{r, include=FALSE}
#Find Bernie before the data was made anonymous
#filter(test, ID == "Bernie")

#filter out Bernie form the data_test dataframe
sanders <- filter(data_test, ados1 == "7", verbalIQ == "28")

```

```{r, include=FALSE}
detach(package:plyr)
library(dplyr)

# Filtering out TD participants
TD_full <- data %>% 
  filter(diagnosis == "TD")

# As numeric
TD_full$visit <- as.numeric(TD_full$visit)

# Grouping by visit and summarising to means of required variables
TD_average <- TD_full %>% 
  group_by(visit) %>%
  summarise(mean_TD_mlu = mean(CHI_MLU), verbalIQ = mean(verbalIQ), mot_mlu = mean(MOT_MLU), ados = mean(ados1))

#lmer(CHI_MLU ~ visit + I(visit^2) + verbalIQ + MOT_MLU + ados1 + (1 + visit + I(visit^2) | ID)
#summary(zeus)

#Calculate TD ChildMlu using the model 
intercept <- -1.286345
visit <- 0.452264
visit2 <- -0.036199
ados <- 0.002641  
MOT_MLU <- 0.286893
verbalIQ <- 0.059322

TD1 <- intercept + visit*1 + (visit2*1^2) + ados*TD_average$ados[1] + MOT_MLU*TD_average$mot_mlu[1] + verbalIQ*TD_average$verbalIQ[1]

TD2 <- intercept + visit*2 + (visit2*2^2) + ados*TD_average$ados[2] + MOT_MLU*TD_average$mot_mlu[2] + verbalIQ*TD_average$verbalIQ[2]

TD3 <- intercept + visit*3 + (visit2*3^2) + ados*TD_average$ados[3] + MOT_MLU*TD_average$mot_mlu[3] + verbalIQ*TD_average$verbalIQ[3]

TD4 <- intercept + visit*4 + (visit2*4^2) + ados*TD_average$ados[4] + MOT_MLU*TD_average$mot_mlu[4] + verbalIQ*TD_average$verbalIQ[4]

TD5 <- intercept + visit*5 + (visit2*5^2) + ados*TD_average$ados[5] + MOT_MLU*TD_average$mot_mlu[5] + verbalIQ*TD_average$verbalIQ[5]

TD6 <- intercept + visit*6 + (visit2*6^2) + ados*TD_average$ados[6] + MOT_MLU*TD_average$mot_mlu[6] + verbalIQ*TD_average$verbalIQ[6]

TD_bernie <- select(sanders, visit, CHI_MLU)
TD_bernie$TD_avr <- c(TD1, TD2, TD3, TD4, TD5, TD6)
TD_bernie$abs_difference <- TD_bernie$CHI_MLU-TD_bernie$TD_avr
```

```{r}
TD_bernie
```

```{r, include=FALSE}
data_test$ID[data_test$ID == 1] <- 70
data_test$ID[data_test$ID == 2] <- 71
data_test$ID[data_test$ID == 3] <- 72
data_test$ID[data_test$ID == 4] <- 73
data_test$ID[data_test$ID == 5] <- 74
data_test$ID[data_test$ID == 6] <- 75
data <- data[,-1]
data_test <- data_test[,-1]

data_with_all <- rbind(data, data_test)

data_without_bernie = filter(data_with_all, !(ID == "71" & visit == 6))
data_bernie_6 = filter(data_with_all, (ID == "71" & visit == 6))
bernie_model = lmer(CHI_MLU ~ visit + I(visit^2) + verbalIQ + MOT_MLU + ados1 + ( 1 + visit + I(visit^2) | ID), data = data_without_bernie, REML=FALSE)
predict(bernie_model, newdata = data_bernie_6, allow.new.levels = TRUE)
sanders$CHI_MLU[sanders$visit == 6]
```

If comparing to model calculations of MLU at each visit, Bernie's MLU exceeds the average (model prediction) value for TD children consistently at each visit. As can be read from the table, the absolute difference ranges from roughly 0.58 (at visit 1) to 1.23 (at visit 3) in Bernie's favor. 

Bernie performs above expectation at visit 6 when using all other available data. While the model predicts Bernie's MLU to be 3.13 at visit 6 while his actual MLU was measured to be 3.45. Bernie's general performance and the model predictions of the average TD is illustrated in the plot below (red being Bernie, blue being TD).

```{r, include=FALSE}
#Plotting Bernies predictions versus actualt performance 
#theme_set(theme_bw())
bernie_plot <- ggplot(data = TD_bernie, aes(x = visit)) + geom_point(aes(y=CHI_MLU), colour="red") + 
  geom_point(aes(y=TD_avr), colour="blue") +
  geom_line(aes(y=CHI_MLU), colour="red", linetype = 2) + 
  geom_line(aes(y=TD_avr), colour="blue", linetype = 2) +
  xlab('Visit') +
  ylab("Mean Lenght of Utterance")
```

```{r}
bernie_plot
```

